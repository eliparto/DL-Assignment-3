{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "3e142c4b-f336-4cc6-9245-e3ea304104b3",
      "metadata": {
        "id": "3e142c4b-f336-4cc6-9245-e3ea304104b3"
      },
      "source": [
        "# DL Assignment 3: Group 38"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6cf2c273-b6f6-48f8-97d7-79a007c797a1",
      "metadata": {
        "id": "6cf2c273-b6f6-48f8-97d7-79a007c797a1"
      },
      "source": [
        "## Part 1"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bf13726d-c49e-476a-ab93-ca078fb9b032",
      "metadata": {
        "id": "bf13726d-c49e-476a-ab93-ca078fb9b032"
      },
      "source": [
        "Q1:\\\n",
        "The general function for the output tensor is as follows: (source: https://pytorch.org/docs/stable/generated/torch.nn.Conv2d.html)\n",
        "\n",
        "$$\n",
        "\\text{out}(N_i, C_{out_j}) = \\cancel{\\text{bias}(C_{out_j})} + \\sum_{j}{C_{in} - 1}\\text{weight}(C_{out_j}, k) * \\text{input}(N_i,k)\n",
        "$$\n",
        "\n",
        "This can be expressed through the following pseudocode:\n",
        "```\n",
        "NOT DONE!\n",
        "for n in batch_size:\n",
        "    for c_out in channels_out:\n",
        "        # output_tensor[n, c_out] = sum\n",
        "        sum = 0\n",
        "        for c_in in channels_in:\n",
        "            sum += weight[c_out, c_in) * input[b, c_in]\n",
        "        output[n, c_out] = sum\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "33427512-ad6a-4a1d-8c24-deab956386fb",
      "metadata": {
        "id": "33427512-ad6a-4a1d-8c24-deab956386fb"
      },
      "source": [
        "Q2:\\\n",
        "The output height and width for a layer $l$ can be expressed as follows:\n",
        "\n",
        "$$\n",
        "n_{H|W}^{[l]} = \\frac{n_{H|W}^{[l-1]} + 2p^{[l-1]}-f^{[l-1]}}{s^{[l-1]}} + 1\n",
        "$$\n",
        "\n",
        "with $s$ representing stride, $p$ representing padding, and $f$ representing filter size."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "19a63629-0bae-47cf-8141-fdb132df4455",
      "metadata": {
        "id": "19a63629-0bae-47cf-8141-fdb132df4455"
      },
      "source": [
        "Q3:\\\n",
        "A pseudocode implementation for the unfold function is as follows:\n",
        "see: https://pytorch.org/docs/stable/generated/torch.nn.Unfold.html#torch.nn.Unfold\n",
        "```\n",
        "func unfold(input_tensor, p = padding, s = stride, k = kernel)\n",
        "    # Add padding\n",
        "    padded_tensor = zero_tensor[batches, channels, height+2p, width+2p]\n",
        "\n",
        "    # Insert input tensor in zero tensor with padding\n",
        "    padded_tensor[:, :, p:height+p, width+p] = input_tensor\n",
        "\n",
        "    # Find and store patches\n",
        "    patches = []\n",
        "    unfold_tensor = [kernel_size]\n",
        "    # Slide window accross input and store\n",
        "    for patch in padded_tensor:\n",
        "        Add patch to patches\n",
        "\n",
        "    # Iterate along patches at every index of the kernel window\n",
        "    for index in unfold_tensor:\n",
        "        values = []\n",
        "        for patch in patches:\n",
        "            for patch_value in patch[index]\n",
        "            append patch_value to values\n",
        "        unfold_tensor[index] = values\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "508f97e5-ec4c-4ceb-a560-98ac3e875285",
      "metadata": {
        "id": "508f97e5-ec4c-4ceb-a560-98ac3e875285"
      },
      "source": [
        "Code below:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "183dd219-adca-414a-83ca-01b7469271d5",
      "metadata": {
        "id": "183dd219-adca-414a-83ca-01b7469271d5"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "import torch.nn.functional as F\n",
        "from tqdm.notebook import tqdm\n",
        "import math"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a945e48e-ae33-4beb-8509-8720ebf7fb09",
      "metadata": {
        "id": "a945e48e-ae33-4beb-8509-8720ebf7fb09"
      },
      "source": [
        "### Manual Unfolding"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "87f6028f-0a8b-425b-876e-d990994251fb",
      "metadata": {
        "id": "87f6028f-0a8b-425b-876e-d990994251fb"
      },
      "outputs": [],
      "source": [
        "# Manual Conv w/ manual unfolding\n",
        "class Conv2D(nn.Module):\n",
        "\n",
        "    def __init__(self, in_channels, out_channels, kernel_size = (3,3),\n",
        "                 stride = 1, padding = 1):\n",
        "        super(Conv2D, self).__init__()\n",
        "        self.in_channels = in_channels\n",
        "        self.out_channels = out_channels\n",
        "        self.kernel_size = kernel_size\n",
        "        self.stride = stride\n",
        "        self.padding = padding\n",
        "\n",
        "    def forward(self, input_batch):\n",
        "        b, c, h, w = input_batch.size()\n",
        "\n",
        "        # Add padding\n",
        "        tensor_pad = torch.full((b, c, h+2*self.padding, w+2*self.padding), 0.0)\n",
        "        tensor_pad[:, :, self.padding:h+self.padding, self.padding:w+self.padding] = input_batch\n",
        "\n",
        "        # Extract patches\n",
        "        _, _, h_pad, w_pad = tensor_pad.size()\n",
        "        x_steps = w_pad - self.kernel_size[0] / self.stride + 1\n",
        "        y_steps = h_pad - self.kernel_size[1] / self.stride + 1\n",
        "\n",
        "        # patches = torch.zeros(1,1024,3,3)\n",
        "        patches = torch.zeros(b, h * w, self.kernel_size[0], self.kernel_size[1])\n",
        "        for batch in range(b):\n",
        "            temp_patches = torch.empty(c, self.kernel_size[0], self.kernel_size[1])\n",
        "            for channel in range(c):\n",
        "                for x in range (0, int(x_steps), self.stride):\n",
        "                    for y in range(0, int(y_steps), self.stride):\n",
        "                        patch = tensor_pad[0,0,x:x+3,y:y+3]\n",
        "                        patch = torch.unsqueeze(patch, dim = 0)\n",
        "                        temp_patches = torch.cat((temp_patches, patch))\n",
        "            temp_patches = temp_patches[1:] # All individual patches in one batch\n",
        "            temp_patches = torch.unsqueeze(temp_patches, dim = 0)\n",
        "            print(temp_patches.size())\n",
        "            patches = torch.cat((patches, temp_patches), dim = 0) # Insert patches of one batch into tensor of all patches\n",
        "        patches = patches[1:]\n",
        "\n",
        "        # Fill in values in unfold tensor\n",
        "        unfold = torch.zeros(b,self.kernel_size[0] * self.kernel_size[1], h * w)\n",
        "        index = 0\n",
        "        for batch in range(b):\n",
        "            for x in range(self.kernel_size[0]):\n",
        "                for y in range(self.kernel_size[1]):\n",
        "                    unfold[batch][index] = torch.unsqueeze(patches, dim = 0)[0,0,:,x,y] # Make tensor of all values at an index of the oatches\n",
        "                    index += 1\n",
        "\n",
        "        return unfold, patches, tensor_pad"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "93e24d43-95d7-4f16-ab76-06dcc8487032",
      "metadata": {
        "id": "93e24d43-95d7-4f16-ab76-06dcc8487032",
        "outputId": "3aaf6612-2fbf-4276-bee1-79d932f27be3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Own output:\n",
            "\n",
            "torch.Size([1, 1024, 3, 3])\n",
            "tensor([[[0.0000, 0.0000, 0.0000,  ..., 0.9192, 0.5909, 0.6573],\n",
            "         [0.0000, 0.0000, 0.0000,  ..., 0.5909, 0.6573, 0.9361],\n",
            "         [0.0000, 0.0000, 0.0000,  ..., 0.6573, 0.9361, 0.0000],\n",
            "         ...,\n",
            "         [0.0000, 0.8208, 0.7676,  ..., 0.0000, 0.0000, 0.0000],\n",
            "         [0.8208, 0.7676, 0.3601,  ..., 0.0000, 0.0000, 0.0000],\n",
            "         [0.7676, 0.3601, 0.8680,  ..., 0.0000, 0.0000, 0.0000]]])\n",
            "\n",
            "PyTorch unfold output:\n",
            "\n",
            "tensor([[[0.0000, 0.0000, 0.0000,  ..., 0.9192, 0.5909, 0.6573],\n",
            "         [0.0000, 0.0000, 0.0000,  ..., 0.5909, 0.6573, 0.9361],\n",
            "         [0.0000, 0.0000, 0.0000,  ..., 0.6573, 0.9361, 0.0000],\n",
            "         ...,\n",
            "         [0.0000, 0.8208, 0.7676,  ..., 0.0000, 0.0000, 0.0000],\n",
            "         [0.8208, 0.7676, 0.3601,  ..., 0.0000, 0.0000, 0.0000],\n",
            "         [0.7676, 0.3601, 0.8680,  ..., 0.0000, 0.0000, 0.0000]]])\n"
          ]
        }
      ],
      "source": [
        "conv = Conv2D(1,1)\n",
        "input_tensor = torch.rand(1,1,32,32)\n",
        "print(\"Own output:\\n\")\n",
        "out, patch, padded = conv(input_tensor)\n",
        "print(out)\n",
        "print(\"\\nPyTorch unfold output:\\n\")\n",
        "unfoldFunc = torch.nn.Unfold((3,3), dilation = 1, padding = 1, stride = 1)\n",
        "print(unfoldFunc(input_tensor))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "28b48f42-034f-4921-9878-a58d0968d706",
      "metadata": {
        "id": "28b48f42-034f-4921-9878-a58d0968d706"
      },
      "source": [
        "### Using nn.Unfold"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "13624b63-ae49-4808-90f9-3c07e253ad84",
      "metadata": {
        "id": "13624b63-ae49-4808-90f9-3c07e253ad84"
      },
      "source": [
        "#### Module Implementation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1f3d7350-2dfa-4f24-8039-45bf94d5be51",
      "metadata": {
        "id": "1f3d7350-2dfa-4f24-8039-45bf94d5be51"
      },
      "outputs": [],
      "source": [
        "class Conv2D(nn.Module):\n",
        "\n",
        "    def __init__(self, in_channels, out_channels, kernel, kernel_size = (3,3),\n",
        "                 stride = 1, padding = 1):\n",
        "        super(Conv2D, self).__init__()\n",
        "        self.in_channels = in_channels\n",
        "        self.out_channels = out_channels\n",
        "        self.kernel = kernel\n",
        "        self.kernel_size = kernel_size\n",
        "        self.stride = stride\n",
        "        self.padding = padding\n",
        "\n",
        "    def forward(self, input_batch):\n",
        "\n",
        "        b, c, h, w = input_batch.size()\n",
        "        output_size = self.calcSize(b, h, w)\n",
        "\n",
        "        # Folding and unfolding functions\n",
        "        unfoldFunc = nn.Unfold(kernel_size = self.kernel_size,\n",
        "                               padding = self.padding, stride = self.stride)\n",
        "        # Don't use the prior kernel/padding/stride params when folding\n",
        "        foldFunc = nn.Fold(output_size = (output_size[2], output_size[3]), kernel_size = (1, 1))\n",
        "\n",
        "        # Unfold input batch\n",
        "        unfold = unfoldFunc(input_batch)\n",
        "\n",
        "        # Multiply input batch with weights -> (XW) and fold into output shape\n",
        "        # Appropriate transformations are applied for correct output shapes\n",
        "        output_unfold = unfold.transpose(1,2).matmul(self.kernel.view(self.kernel.size(0), -1).t()).transpose(1, 2)\n",
        "        output = foldFunc(output_unfold)\n",
        "        assert list(output.size()) == output_size, f'{list(output.size())} generated. Need {output_size}'\n",
        "\n",
        "        return output_unfold\n",
        "\n",
        "    # Calculate the correct output size\n",
        "    def calcSize(self, b, h, w):\n",
        "        size_x = (h + 2 * self.padding - self.kernel_size[0]) / self.stride + 1\n",
        "        size_y = (w + 2 * self.padding - self.kernel_size[0]) / self.stride + 1\n",
        "\n",
        "        return [b, self.out_channels, int(size_x), int(size_y)]\n",
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "199e898c-defe-44f4-bfc7-23ae98248696",
      "metadata": {
        "id": "199e898c-defe-44f4-bfc7-23ae98248696"
      },
      "outputs": [],
      "source": [
        "# Test output\n",
        "input_batch = torch.randn(16, 3, 32, 32)\n",
        "weights = torch.randn(3, 3, 3, 3)\n",
        "conv = Conv2D(3, 3, weights)\n",
        "output_batch = conv(input_batch)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "77f81179-b55c-42d2-800b-8d4a78c40c72",
      "metadata": {
        "id": "77f81179-b55c-42d2-800b-8d4a78c40c72"
      },
      "source": [
        "#### Function Implementation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6547dcfc-f183-4923-bf17-610339b6fcc8",
      "metadata": {
        "id": "6547dcfc-f183-4923-bf17-610339b6fcc8"
      },
      "outputs": [],
      "source": [
        "class Conv2DFunc(torch.autograd.Function):\n",
        "\n",
        "    @staticmethod\n",
        "    def forward(ctx, input_batch, kernel, stride = 1, padding = 1):\n",
        "        \"\"\"\n",
        "        Receive tensor containing input -> return tensor contaiing output.\n",
        "        ctx: context object containing info for backward pass.\n",
        "        Cache objects for use in backward pass using: ctx.save_for_backward\n",
        "        \"\"\"\n",
        "\n",
        "        # Forward pass\n",
        "        b, c, h, w = input_batch.size()\n",
        "\n",
        "        # Calculate correct output dimensions\n",
        "        size_x = (h + 2 * padding - kernel.size(2)) / stride + 1\n",
        "        size_y = (w + 2 * padding - kernel.size(3)) / stride + 1\n",
        "        output_size = [b, kernel.size(0), int(size_x), int(size_y)]\n",
        "\n",
        "        # Folding and unfolding functions\n",
        "        unfoldFunc = nn.Unfold(kernel_size = (kernel.size(2), kernel.size(3)),\n",
        "                               padding = padding, stride = stride)\n",
        "        # Don't use the prior kernel/padding/stride params when folding\n",
        "        foldFunc = nn.Fold(output_size = (output_size[2], output_size[3]), kernel_size = (1, 1))\n",
        "\n",
        "        # Unfold -> tensor ops -> fold\n",
        "        U = unfoldFunc(input_batch)\n",
        "        UW = U.transpose(1,2).matmul(kernel.view(kernel.size(0), -1).t()).transpose(1, 2)\n",
        "        Y = foldFunc(UW)\n",
        "        assert list(Y.size()) == output_size, f'{list(Y.size())} generated. Need {output_size}'\n",
        "\n",
        "        # Store for backward pass\n",
        "        W = kernel.view(kernel.size(0), -1) # Format to follow computation graph shape\n",
        "        ctx.save_for_backward(input_batch, kernel, U, W)\n",
        "        ctx.stride = stride\n",
        "        ctx.padding = padding\n",
        "\n",
        "        return Y\n",
        "\n",
        "    @staticmethod\n",
        "    def backward(ctx, grad_output):\n",
        "        \"\"\"\n",
        "        Generate tensor containing gradient of loss w.r.t. output.\n",
        "        Compute gradient of loss w.r.t. input\n",
        "        \"\"\"\n",
        "\n",
        "        # Retrieve stored objects and dimensions\n",
        "        input_batch, kernel, U, W = ctx.saved_tensors\n",
        "        b_in, c_in, h_in, w_in = input_batch.size()\n",
        "        b_out, c_out, h_out, w_out = grad_output.size()\n",
        "        stride = ctx.stride\n",
        "        padding = ctx.padding\n",
        "\n",
        "        # Backward pass\n",
        "        # Reshape Y^nabla to pre-folded shape (Y'^nabla)\n",
        "        Y_nabla = grad_output.view(b_out, c_out, (h_out * w_out)).transpose(1,2)\n",
        "\n",
        "        # W^nabla = U x Y^nabla\n",
        "        W_nabla = U.matmul(Y_nabla)\n",
        "        # U^nabla = Y^nabla x W.T\n",
        "        U_nabla = Y_nabla.matmul(W.view(W.size(0), -1)).transpose(1,2)\n",
        "        assert U_nabla.size() == U.size(), f'U_nabla: {U_nabla.size()} generated. Need {U.size()}'\n",
        "\n",
        "        # Fold U^nabla into X^nabla\n",
        "        foldFunc = nn.Fold(output_size=(32, 32), kernel_size=(3, 3), dilation=1,\n",
        "                           padding=(padding, padding), stride=stride)\n",
        "        X_nabla = foldFunc(U_nabla)\n",
        "        assert X_nabla.size() == input_batch.size(), f'X_nabla: {X_nabla.size()} generated. Need {input_batch.size()}'\n",
        "\n",
        "        return X_nabla, W_nabla"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "adfb48cf-c366-42c5-90e6-331afb293d8b",
      "metadata": {
        "id": "adfb48cf-c366-42c5-90e6-331afb293d8b"
      },
      "outputs": [],
      "source": [
        "# Test output\n",
        "input_batch = torch.randn(16, 3, 32, 32, requires_grad = True)\n",
        "kernel = torch.randn(3, 3, 3, 3)\n",
        "output_batch = Conv2DFunc.apply(input_batch, kernel)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c18b992d-6bb3-44f9-a60d-68b1e65e09db",
      "metadata": {
        "id": "c18b992d-6bb3-44f9-a60d-68b1e65e09db",
        "outputId": "9b21a87c-f9fd-451e-b5de-58932e25faac"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(-588.8141, grad_fn=<SumBackward0>)\n"
          ]
        }
      ],
      "source": [
        "# Example loss function\n",
        "loss_fn = torch.sum\n",
        "loss = loss_fn(output_batch)\n",
        "print(loss.sum())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "403228e0-6484-4016-89c8-125d82d1d3e0",
      "metadata": {
        "id": "403228e0-6484-4016-89c8-125d82d1d3e0"
      },
      "outputs": [],
      "source": [
        "loss.backward()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "423f6db3-295b-4e46-9e57-20910d185311",
      "metadata": {
        "id": "423f6db3-295b-4e46-9e57-20910d185311"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Section 2\n"
      ],
      "metadata": {
        "id": "ITwMuaZHRyE0"
      },
      "id": "ITwMuaZHRyE0"
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q7"
      ],
      "metadata": {
        "id": "LV-2gxxpR5fD"
      },
      "id": "LV-2gxxpR5fD"
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torchvision\n",
        "from torchvision.transforms import ToTensor\n",
        "from torch.utils.data import DataLoader, random_split, Dataset\n",
        "from torch import nn, optim"
      ],
      "metadata": {
        "id": "dMY7xR-2R6cJ"
      },
      "id": "dMY7xR-2R6cJ",
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Load MNIST Data:"
      ],
      "metadata": {
        "id": "wruk4U3bSDdQ"
      },
      "id": "wruk4U3bSDdQ"
    },
    {
      "cell_type": "code",
      "source": [
        "transform = ToTensor()\n",
        "train_dataset = torchvision.datasets.MNIST(root=\"./data\", train=True, download=True, transform=transform)\n",
        "test_dataset = torchvision.datasets.MNIST(root=\"./data\", train=False, download=True, transform=transform)"
      ],
      "metadata": {
        "id": "JMvoqUsGSFSo"
      },
      "id": "JMvoqUsGSFSo",
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Convert to a List and Split -> Split into Training and Validation Sets, 50000 images for training, 10000 f:or validation"
      ],
      "metadata": {
        "id": "sBbUSBKVSOBU"
      },
      "id": "sBbUSBKVSOBU"
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert to a list of tuples for easier splitting\n",
        "train_list = [(image, label) for image, label in train_dataset]\n",
        "\n",
        "# Split the data into training and validation sets\n",
        "train_list, val_list = train_list[:50000], train_list[50000:]\n"
      ],
      "metadata": {
        "id": "6kWf-XkYSUou"
      },
      "id": "6kWf-XkYSUou",
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Create a Custom dataaset class to put the lists we split back together"
      ],
      "metadata": {
        "id": "k3i0LG4v7DPH"
      },
      "id": "k3i0LG4v7DPH"
    },
    {
      "cell_type": "code",
      "source": [
        "class CustomDataset(Dataset):\n",
        "    def __init__(self, data_list):\n",
        "        self.data_list = data_list\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data_list)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.data_list[idx]\n",
        "\n",
        "# Create Dataset objects\n",
        "train_data = CustomDataset(train_list)\n",
        "val_data = CustomDataset(val_list)\n"
      ],
      "metadata": {
        "id": "-iv3JWKe7JLa"
      },
      "id": "-iv3JWKe7JLa",
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Create data loaders, in order to load the data in batches of 16 images\n",
        "Shuffle the training data to ensure each epoch uses a different data order"
      ],
      "metadata": {
        "id": "jeO7yMpwSaHv"
      },
      "id": "jeO7yMpwSaHv"
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 16\n",
        "\n",
        "train_loader = DataLoader(train_data, batch_size=batch_size, shuffle=True)\n",
        "val_loader = DataLoader(val_data, batch_size=batch_size, shuffle=False)\n",
        "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)"
      ],
      "metadata": {
        "id": "8WPg80zvSiv1"
      },
      "id": "8WPg80zvSiv1",
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Define the CNN architecture"
      ],
      "metadata": {
        "id": "BKXTa9I8Sulv"
      },
      "id": "BKXTa9I8Sulv"
    },
    {
      "cell_type": "code",
      "source": [
        "class CNN(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(CNN, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(1, 16, kernel_size=3, stride=1, padding=1)\n",
        "        self.conv2 = nn.Conv2d(16, 32, kernel_size=3, stride=1, padding=1)\n",
        "        self.conv3 = nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1)\n",
        "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "        self.fc = nn.Linear(64 * 3 * 3, 10)\n",
        "        self.relu = nn.ReLU()\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.relu(self.conv1(x))\n",
        "        x = self.pool(x)\n",
        "        x = self.relu(self.conv2(x))\n",
        "        x = self.pool(x)\n",
        "        x = self.relu(self.conv3(x))\n",
        "        x = self.pool(x)\n",
        "        x = x.view(x.size(0), -1)  # Flatten\n",
        "        x = self.fc(x)\n",
        "        return x\n"
      ],
      "metadata": {
        "id": "Vbp1VSC8S1U2"
      },
      "id": "Vbp1VSC8S1U2",
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Set up training components (lr = 0.01) - we also need to put batch size (we already set it earlier as 16), we remove the momentum that SGD originally has.\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "zjWEi2Pg7amr"
      },
      "id": "zjWEi2Pg7amr"
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "model = CNN().to(device)\n",
        "loss_function = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)  # Adam with default parameters\n"
      ],
      "metadata": {
        "id": "28FlG61578bs"
      },
      "id": "28FlG61578bs",
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        ":Training Loop: Process each batch, calculate loss, and update weights.The loop processes batches from the data loader. (we also included validation so we can already used it for q 8)"
      ],
      "metadata": {
        "id": "afoeGaBVTCBP"
      },
      "id": "afoeGaBVTCBP"
    },
    {
      "cell_type": "code",
      "source": [
        "NUM_EPOCHS = 15\n",
        "train_accuracies = []\n",
        "val_accuracies = []\n",
        "\n",
        "for epoch in range(NUM_EPOCHS):\n",
        "    # Training phase\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "    correct_train = 0\n",
        "    total_train = 0\n",
        "\n",
        "    for inputs, labels in train_loader:  # Use DataLoader for batches\n",
        "        inputs, labels = inputs.to(device), labels.to(device)\n",
        "\n",
        "        optimizer.zero_grad()  # Reset gradients\n",
        "        outputs = model(inputs)  # Forward pass\n",
        "        loss = loss_function(outputs, labels)  # Calculate loss\n",
        "        loss.backward()  # Backward pass\n",
        "        optimizer.step()  # Update weights\n",
        "\n",
        "        running_loss += loss.item()\n",
        "        _, predicted = torch.max(outputs, 1)\n",
        "        total_train += labels.size(0)\n",
        "        correct_train += (predicted == labels).sum().item()\n",
        "\n",
        "    train_accuracy = correct_train / total_train\n",
        "    train_accuracies.append(train_accuracy)\n",
        "\n",
        "    # Validation phase\n",
        "    model.eval()\n",
        "    correct_val = 0\n",
        "    total_val = 0\n",
        "    with torch.no_grad():\n",
        "        for inputs, labels in val_loader:  # Use DataLoader for batches\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "\n",
        "            outputs = model(inputs)\n",
        "            _, predicted = torch.max(outputs, 1)\n",
        "            total_val += labels.size(0)\n",
        "            correct_val += (predicted == labels).sum().item()\n",
        "\n",
        "    val_accuracy = correct_val / total_val\n",
        "    val_accuracies.append(val_accuracy)\n",
        "\n",
        "    print(f\"Epoch {epoch + 1}/{NUM_EPOCHS}: \"\n",
        "          f\"Loss: {running_loss / len(train_loader):.4f}, \"\n",
        "          f\"Train Accuracy: {train_accuracy * 100:.2f}%, \"\n",
        "          f\"Validation Accuracy: {val_accuracy * 100:.2f}%\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MV1uUjTETE8J",
        "outputId": "a87acb14-3016-4b54-b910-2ef01f7e31dd"
      },
      "id": "MV1uUjTETE8J",
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/15: Loss: 0.0067, Train Accuracy: 99.80%, Validation Accuracy: 98.90%\n",
            "Epoch 2/15: Loss: 0.0060, Train Accuracy: 99.82%, Validation Accuracy: 98.86%\n",
            "Epoch 3/15: Loss: 0.0056, Train Accuracy: 99.84%, Validation Accuracy: 98.85%\n",
            "Epoch 4/15: Loss: 0.0055, Train Accuracy: 99.84%, Validation Accuracy: 98.72%\n",
            "Epoch 5/15: Loss: 0.0045, Train Accuracy: 99.89%, Validation Accuracy: 98.82%\n",
            "Epoch 6/15: Loss: 0.0036, Train Accuracy: 99.91%, Validation Accuracy: 98.85%\n",
            "Epoch 7/15: Loss: 0.0039, Train Accuracy: 99.90%, Validation Accuracy: 98.60%\n",
            "Epoch 8/15: Loss: 0.0038, Train Accuracy: 99.90%, Validation Accuracy: 98.92%\n",
            "Epoch 9/15: Loss: 0.0033, Train Accuracy: 99.91%, Validation Accuracy: 98.90%\n",
            "Epoch 10/15: Loss: 0.0028, Train Accuracy: 99.94%, Validation Accuracy: 98.88%\n",
            "Epoch 11/15: Loss: 0.0027, Train Accuracy: 99.93%, Validation Accuracy: 98.84%\n",
            "Epoch 12/15: Loss: 0.0025, Train Accuracy: 99.94%, Validation Accuracy: 98.96%\n",
            "Epoch 13/15: Loss: 0.0017, Train Accuracy: 99.98%, Validation Accuracy: 98.92%\n",
            "Epoch 14/15: Loss: 0.0014, Train Accuracy: 99.98%, Validation Accuracy: 98.91%\n",
            "Epoch 15/15: Loss: 0.0012, Train Accuracy: 99.98%, Validation Accuracy: 98.87%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q8"
      ],
      "metadata": {
        "id": "5yhZngR2pM4i"
      },
      "id": "5yhZngR2pM4i"
    },
    {
      "cell_type": "markdown",
      "source": [
        "Implementing the CNN (with the pre-defined given architectrure)"
      ],
      "metadata": {
        "id": "mfUshsTTpP-_"
      },
      "id": "mfUshsTTpP-_"
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.9"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}