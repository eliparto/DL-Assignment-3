{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3e142c4b-f336-4cc6-9245-e3ea304104b3",
   "metadata": {},
   "source": [
    "# DL Assignment 3: Group 38"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cf2c273-b6f6-48f8-97d7-79a007c797a1",
   "metadata": {},
   "source": [
    "## Part 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf13726d-c49e-476a-ab93-ca078fb9b032",
   "metadata": {},
   "source": [
    "Q1:\\\n",
    "The general function for the output tensor is as follows: (source: https://pytorch.org/docs/stable/generated/torch.nn.Conv2d.html)\n",
    "\n",
    "$$ \n",
    "\\text{out}(N_i, C_{out_j}) = \\cancel{\\text{bias}(C_{out_j})} + \\sum_{j}{C_{in} - 1}\\text{weight}(C_{out_j}, k) * \\text{input}(N_i,k)\n",
    "$$\n",
    "\n",
    "This can be expressed through the following pseudocode:\n",
    "```\n",
    "NOT DONE!\n",
    "for n in batch_size:\n",
    "    for c_out in channels_out:\n",
    "        # output_tensor[n, c_out] = sum\n",
    "        sum = 0 \n",
    "        for c_in in channels_in:\n",
    "            sum += weight[c_out, c_in) * input[b, c_in]\n",
    "        output[n, c_out] = sum\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33427512-ad6a-4a1d-8c24-deab956386fb",
   "metadata": {},
   "source": [
    "Q2:\\\n",
    "The output height and width for a layer $l$ can be expressed as follows:\n",
    "\n",
    "$$\n",
    "n_{H|W}^{[l]} = \\frac{n_{H|W}^{[l-1]} + 2p^{[l-1]}-f^{[l-1]}}{s^{[l-1]}} + 1\n",
    "$$\n",
    "\n",
    "with $s$ representing stride, $p$ representing padding, and $f$ representing filter size."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19a63629-0bae-47cf-8141-fdb132df4455",
   "metadata": {},
   "source": [
    "Q3:\\\n",
    "A pseudocode implementation for the unfold function is as follows:\n",
    "see: https://pytorch.org/docs/stable/generated/torch.nn.Unfold.html#torch.nn.Unfold\n",
    "```\n",
    "func unfold(input):\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "508f97e5-ec4c-4ceb-a560-98ac3e875285",
   "metadata": {},
   "source": [
    "Q4:\\\n",
    "Code below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "183dd219-adca-414a-83ca-01b7469271d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from tqdm.notebook import tqdm\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "id": "87f6028f-0a8b-425b-876e-d990994251fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Conv2D(nn.Module):\n",
    "\n",
    "    def __init__(self, in_channels, out_channels, kernel_size = (3,3),\n",
    "                 stride = 1, padding = 1):\n",
    "        super(Conv2D, self).__init__()\n",
    "        self.in_channels = in_channels\n",
    "        self.out_channels = out_channels\n",
    "        self.kernel_size = kernel_size\n",
    "        self.stride = stride\n",
    "        self.padding = padding\n",
    "\n",
    "    def forward(self, input_batch):\n",
    "        b, c, h, w = input_batch.size()\n",
    "        \n",
    "        # Add padding\n",
    "        tensor_pad = torch.full((b, c, h+2*self.padding, w+2*self.padding), 0.0)\n",
    "        tensor_pad[:, :, self.padding:h+self.padding, self.padding:w+self.padding] = input_batch\n",
    "\n",
    "        # Extract patches\n",
    "        _, _, h_pad, w_pad = tensor_pad.size()\n",
    "        x_steps = w_pad - self.kernel_size[0] / self.stride + 1\n",
    "        y_steps = h_pad - self.kernel_size[1] / self.stride + 1\n",
    "\n",
    "        patches = torch.zeros(1,1024,3,3)\n",
    "        for batch in range(b):\n",
    "            temp_patches = torch.empty(c,3,3)\n",
    "            for channel in range(c):\n",
    "                for x in range (0, int(x_steps), self.stride):\n",
    "                    for y in range(0, int(y_steps), self.stride):\n",
    "                        patch = tensor_pad[0,0,y:y+3,x:x+3]\n",
    "                        patch = torch.unsqueeze(patch, dim = 0)\n",
    "                        temp_patches = torch.cat((temp_patches, patch))\n",
    "            temp_patches = temp_patches[1:] # All individual patches in one batch \n",
    "            temp_patches = torch.unsqueeze(temp_patches, dim = 0)\n",
    "            patches = torch.cat((patches, temp_patches), dim = 0)\n",
    "        patches = patches[1:] # All patches of one batch to all patches\n",
    "\n",
    "        # Fill in values in unfold tensor\n",
    "        unfold = torch.zeros(b,self.kernel_size[0] * self.kernel_size[1], h * w)\n",
    "        index = 0\n",
    "        for batch in range(b):\n",
    "            for x in range(3):\n",
    "                for y in range(3):\n",
    "                    unfold[batch][index] = torch.unsqueeze(patches, dim = 0)[0,0,:,y,x] # Make tensor of all values at an index of the oatches\n",
    "                    index += 1\n",
    "        \n",
    "        # return unfold, ref, input_batch # for debugging\n",
    "        return unfold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "id": "93e24d43-95d7-4f16-ab76-06dcc8487032",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Own output:\n",
      "\n",
      "tensor([[[0.0000, 0.0000, 0.0000,  ..., 0.5136, 0.9149, 0.9109],\n",
      "         [0.0000, 0.0000, 0.0000,  ..., 0.9149, 0.9109, 0.1559],\n",
      "         [0.0000, 0.0000, 0.0000,  ..., 0.9109, 0.1559, 0.0000],\n",
      "         ...,\n",
      "         [0.0000, 0.5743, 0.3518,  ..., 0.0000, 0.0000, 0.0000],\n",
      "         [0.5743, 0.3518, 0.9692,  ..., 0.0000, 0.0000, 0.0000],\n",
      "         [0.3518, 0.9692, 0.7081,  ..., 0.0000, 0.0000, 0.0000]]])\n",
      "\n",
      "PyTorch unfold output:\n",
      "\n",
      "tensor([[[0.0000, 0.0000, 0.0000,  ..., 0.3678, 0.9744, 0.9109],\n",
      "         [0.0000, 0.0000, 0.0000,  ..., 0.9744, 0.9109, 0.7505],\n",
      "         [0.0000, 0.0000, 0.0000,  ..., 0.9109, 0.7505, 0.0000],\n",
      "         ...,\n",
      "         [0.0000, 0.7703, 0.3518,  ..., 0.0000, 0.0000, 0.0000],\n",
      "         [0.7703, 0.3518, 0.2607,  ..., 0.0000, 0.0000, 0.0000],\n",
      "         [0.3518, 0.2607, 0.6012,  ..., 0.0000, 0.0000, 0.0000]]])\n"
     ]
    }
   ],
   "source": [
    "conv = Conv2D(1,1)\n",
    "input_tensor = torch.rand(1,1,32,32)\n",
    "print(\"Own output:\\n\")\n",
    "print(conv(input_tensor))\n",
    "print(\"\\nPyTorch unfold output:\\n\")\n",
    "unfoldFunc = torch.nn.Unfold((3,3), dilation = 1, padding = 1, stride = 1)\n",
    "print(unfoldFunc(input_tensor))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31e15eb9-03d1-41af-bec8-34b3f98c2e5f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
